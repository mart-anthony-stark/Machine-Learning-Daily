#  Calculating Loss with Categorical Cross-Entropy
'''
solving for x
e**x = b
'''
import numpy as np
import math

softmax_output = [0.7, 0.1, 0.2]
target_output = [1, 0, 0]


b = 5.2

print(np.log(b))
print(math.e**1.6486586255873816)
