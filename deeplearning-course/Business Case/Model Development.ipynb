{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8203d3c3",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aced51",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15423bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac21c3f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f768191",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "\n",
    "validation_inputs = npz['inputs'].astype(float)\n",
    "validation_targets = npz['targets'].astype(int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "\n",
    "test_inputs = npz['inputs'].astype(float)\n",
    "test_targets = npz['targets'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660e8da",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed18de24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 3s - loss: 0.4672 - accuracy: 0.8405 - val_loss: 0.2992 - val_accuracy: 0.8993 - 3s/epoch - 86ms/step\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.3220 - accuracy: 0.8852 - val_loss: 0.2718 - val_accuracy: 0.9060 - 286ms/epoch - 8ms/step\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.2930 - accuracy: 0.8908 - val_loss: 0.2543 - val_accuracy: 0.9083 - 287ms/epoch - 8ms/step\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.2778 - accuracy: 0.8986 - val_loss: 0.2459 - val_accuracy: 0.9128 - 274ms/epoch - 8ms/step\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.2688 - accuracy: 0.8980 - val_loss: 0.2545 - val_accuracy: 0.9172 - 294ms/epoch - 8ms/step\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.2630 - accuracy: 0.9014 - val_loss: 0.2450 - val_accuracy: 0.9150 - 293ms/epoch - 8ms/step\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.2591 - accuracy: 0.9030 - val_loss: 0.2420 - val_accuracy: 0.9195 - 285ms/epoch - 8ms/step\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.2506 - accuracy: 0.9047 - val_loss: 0.2482 - val_accuracy: 0.9150 - 279ms/epoch - 8ms/step\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.2457 - accuracy: 0.9086 - val_loss: 0.2529 - val_accuracy: 0.8993 - 286ms/epoch - 8ms/step\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.2407 - accuracy: 0.9095 - val_loss: 0.2462 - val_accuracy: 0.9150 - 308ms/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x210a465d910>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_layer_size = 100\n",
    "output_size = 2\n",
    "\n",
    "# tensorboard callback\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "        tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "])\n",
    "\n",
    "# sparse_categorical_crossentropy - performs one-hot encoding to labels\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 100\n",
    "MAX_EPOCHS = 100\n",
    "# Early stopping mechanism(avoid overfitting)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "\n",
    "# feed training data\n",
    "model.fit(train_inputs, train_targets,\n",
    "          batch_size=BATCH_SIZE, epochs=MAX_EPOCHS,\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          callbacks=[early_stopping],\n",
    "            verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5610d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 5ms/step - loss: 0.2585 - accuracy: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2585211396217346, 0.90625)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n",
    "test_loss, test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
